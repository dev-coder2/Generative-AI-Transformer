{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1 \n",
    "### Builded a code for a perceptron(i.e. a single neuron and no hidden layers) and AND gate using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND Gate Results:\n",
      "Input: [0 0], Output: 0\n",
      "Input: [0 1], Output: 0\n",
      "Input: [1 0], Output: 0\n",
      "Input: [1 1], Output: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Perceptron class\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.random.rand(input_size + 1)  # +1 for bias\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def activation(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        inputs = np.append(inputs, 1)  # Add bias input\n",
    "        weighted_sum = np.dot(inputs, self.weights)\n",
    "        return self.activation(weighted_sum)\n",
    "    \n",
    "    def train(self, X, y, epochs):\n",
    "        for _ in range(epochs):\n",
    "            for inputs, target in zip(X, y):\n",
    "                prediction = self.predict(inputs)\n",
    "                error = target - prediction\n",
    "                inputs = np.append(inputs, 1)  # Add bias input\n",
    "                self.weights += self.lr * error * inputs\n",
    "\n",
    "# Step 1a: AND gate with perceptron\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "and_gate = Perceptron(input_size=2)\n",
    "and_gate.train(X_and, y_and, epochs=10)\n",
    "print(\"AND Gate Results:\")\n",
    "for x in X_and:\n",
    "    print(f\"Input: {x}, Output: {and_gate.predict(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: XOR gate (Perceptron - will fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Gate Results (Expected to Fail):\n",
      "Input: [0 0], Output: 0\n",
      "Input: [0 1], Output: 1\n",
      "Input: [1 0], Output: 0\n",
      "Input: [1 1], Output: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "xor_gate = Perceptron(input_size=2)\n",
    "xor_gate.train(X_xor, y_xor, epochs=10)\n",
    "print(\"XOR Gate Results (Expected to Fail):\")\n",
    "for x in X_xor:\n",
    "    print(f\"Input: {x}, Output: {xor_gate.predict(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: XOR Gate with Hidden Layer (Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.38594885819722335\n",
      "Epoch 1000, Loss: 0.24413666226883493\n",
      "Epoch 2000, Loss: 0.2079253284415609\n",
      "Epoch 3000, Loss: 0.13970999738353343\n",
      "Epoch 4000, Loss: 0.03726326657706622\n",
      "Epoch 5000, Loss: 0.014050464471479424\n",
      "Epoch 6000, Loss: 0.007946964659346465\n",
      "Epoch 7000, Loss: 0.005387840295770003\n",
      "Epoch 8000, Loss: 0.004023859675334848\n",
      "Epoch 9000, Loss: 0.0031886150667136738\n",
      "\n",
      "XOR Gate Results (With Gradients):\n",
      "Input: [0 0], Output: [0.05392357]\n",
      "Input: [0 1], Output: [0.95082242]\n",
      "Input: [1 0], Output: [0.95085555]\n",
      "Input: [1 1], Output: [0.05268226]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        self.w1 = np.random.rand(input_size, hidden_size)\n",
    "        self.b1 = np.random.rand(hidden_size)\n",
    "        self.w2 = np.random.rand(hidden_size, output_size)\n",
    "        self.b2 = np.random.rand(output_size)\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Input to Hidden Layer\n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        # Hidden to Output Layer\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # Loss Derivative w.r.t Output (Mean Squared Error Loss)\n",
    "        output_error = y - output  # Error at output layer\n",
    "        output_delta = output_error * self.sigmoid_derivative(output)  # Gradient for output layer\n",
    "        \n",
    "        # Loss Derivative w.r.t Hidden Layer\n",
    "        hidden_error = output_delta.dot(self.w2.T)  # Error propagated to hidden layer\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.a1)  # Gradient for hidden layer\n",
    "        \n",
    "        # Gradients for Weights and Biases\n",
    "        grad_w2 = self.a1.T.dot(output_delta)  # Gradient of w2\n",
    "        grad_b2 = np.sum(output_delta, axis=0)  # Gradient of b2\n",
    "        grad_w1 = X.T.dot(hidden_delta)  # Gradient of w1\n",
    "        grad_b1 = np.sum(hidden_delta, axis=0)  # Gradient of b1\n",
    "\n",
    "        # Update Weights and Biases\n",
    "        self.w2 += self.lr * grad_w2\n",
    "        self.b2 += self.lr * grad_b2\n",
    "        self.w1 += self.lr * grad_w1\n",
    "        self.b1 += self.lr * grad_b1\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "            if epoch % 1000 == 0:  # Print loss every 1000 epochs\n",
    "                loss = np.mean((y - output) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# XOR Gate Example\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=2, output_size=1)\n",
    "nn.train(X, y, epochs=10000)\n",
    "\n",
    "print(\"\\nXOR Gate Results (With Gradients):\")\n",
    "for x in X:\n",
    "    print(f\"Input: {x}, Output: {nn.forward(x)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Full Adder Using Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.26260185684863363\n",
      "Epoch 1000, Loss: 0.2500011083232543\n",
      "Epoch 2000, Loss: 0.24999879486195606\n",
      "Epoch 3000, Loss: 0.24999701760575196\n",
      "Epoch 4000, Loss: 0.24999544820503505\n",
      "Epoch 5000, Loss: 0.24999386696836395\n",
      "Epoch 6000, Loss: 0.24999209153271068\n",
      "Epoch 7000, Loss: 0.24998993142074885\n",
      "Epoch 8000, Loss: 0.24998714691327106\n",
      "Epoch 9000, Loss: 0.24998339661069074\n",
      "Full Adder Results (Sum and Carry-out):\n",
      "Input: [0 0 0], Sum: [0.50542423], Carry-out: 0\n",
      "Input: [0 0 1], Sum: [0.5005069], Carry-out: 0\n",
      "Input: [0 1 0], Sum: [0.50388127], Carry-out: 0\n",
      "Input: [0 1 1], Sum: [0.49937949], Carry-out: 1\n",
      "Input: [1 0 0], Sum: [0.49985029], Carry-out: 0\n",
      "Input: [1 0 1], Sum: [0.4962411], Carry-out: 1\n",
      "Input: [1 1 0], Sum: [0.49889557], Carry-out: 1\n",
      "Input: [1 1 1], Sum: [0.49595357], Carry-out: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# XOR Gate Implementation (Reused from previous code)\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        self.w1 = np.random.rand(input_size, hidden_size)\n",
    "        self.b1 = np.random.rand(hidden_size)\n",
    "        self.w2 = np.random.rand(hidden_size, output_size)\n",
    "        self.b2 = np.random.rand(output_size)\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Input to Hidden Layer\n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        # Hidden to Output Layer\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        # Loss Derivative w.r.t Output (Mean Squared Error Loss)\n",
    "        output_error = y - output  # Error at output layer\n",
    "        output_delta = output_error * self.sigmoid_derivative(output)  # Gradient for output layer\n",
    "        \n",
    "        # Loss Derivative w.r.t Hidden Layer\n",
    "        hidden_error = output_delta.dot(self.w2.T)  # Error propagated to hidden layer\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.a1)  # Gradient for hidden layer\n",
    "        \n",
    "        # Gradients for Weights and Biases\n",
    "        grad_w2 = self.a1.T.dot(output_delta)  # Gradient of w2\n",
    "        grad_b2 = np.sum(output_delta, axis=0)  # Gradient of b2\n",
    "        grad_w1 = X.T.dot(hidden_delta)  # Gradient of w1\n",
    "        grad_b1 = np.sum(hidden_delta, axis=0)  # Gradient of b1\n",
    "\n",
    "        # Update Weights and Biases\n",
    "        self.w2 += self.lr * grad_w2\n",
    "        self.b2 += self.lr * grad_b2\n",
    "        self.w1 += self.lr * grad_w1\n",
    "        self.b1 += self.lr * grad_b1\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "            if epoch % 1000 == 0:  # Print loss every 1000 epochs\n",
    "                loss = np.mean((y - output) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "\n",
    "# Full Adder Using XOR for Sum and AND-OR for Carry-out\n",
    "\n",
    "# Inputs for Full Adder: A, B, Cin\n",
    "X_full_adder = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "# Output: Sum (S), Carry (Cout)\n",
    "y_full_adder = np.array([[0, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 1]])\n",
    "\n",
    "# Step 1: XOR Gate for Sum calculation\n",
    "sum_nn = NeuralNetwork(input_size=3, hidden_size=3, output_size=1)  # 3 inputs (A, B, Cin)\n",
    "sum_nn.train(X_full_adder, y_full_adder[:, 0:1], epochs=10000)  # Training for Sum only\n",
    "\n",
    "# Step 2: Perceptron for Carry-Out Calculation (using AND-OR logic)\n",
    "class CarryOutPerceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.random.rand(input_size + 1)  # +1 for bias\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        inputs = np.append(inputs, 1)  # Add bias input\n",
    "        weighted_sum = np.dot(inputs, self.weights)\n",
    "        return self.activation(weighted_sum)\n",
    "    \n",
    "    def train(self, X, y, epochs):\n",
    "        for _ in range(epochs):\n",
    "            for inputs, target in zip(X, y):\n",
    "                prediction = self.predict(inputs)\n",
    "                error = target - prediction\n",
    "                inputs = np.append(inputs, 1)  # Add bias input\n",
    "                self.weights += self.lr * error * inputs\n",
    "\n",
    "# Carry-Out perceptron (AND logic to check if carry is 1)\n",
    "X_carry = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "y_carry = np.array([0, 0, 0, 1, 0, 1, 1, 1])\n",
    "\n",
    "carry_out_nn = CarryOutPerceptron(input_size=3)\n",
    "carry_out_nn.train(X_carry, y_carry, epochs=10000)\n",
    "\n",
    "# Step 3: Full Adder Prediction using XOR and Carry-out Perceptron\n",
    "print(\"Full Adder Results (Sum and Carry-out):\")\n",
    "for x in X_full_adder:\n",
    "    sum_output = sum_nn.forward(x)  # Get the Sum using XOR gate\n",
    "    carry_output = carry_out_nn.predict(x)  # Get the Carry-out using AND-OR perceptron\n",
    "    print(f\"Input: {x}, Sum: {sum_output}, Carry-out: {carry_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Combine the adders into a ripple carry adder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.37284115196441137\n",
      "Epoch 1000, Loss: 0.25000631304114274\n",
      "Epoch 2000, Loss: 0.25000012611332895\n",
      "Epoch 3000, Loss: 0.24999553971954408\n",
      "Epoch 4000, Loss: 0.24999151353760551\n",
      "Epoch 5000, Loss: 0.24998754738631207\n",
      "Epoch 6000, Loss: 0.24998331472451557\n",
      "Epoch 7000, Loss: 0.24997851726765863\n",
      "Epoch 8000, Loss: 0.2499727965379252\n",
      "Epoch 9000, Loss: 0.24996563264739155\n",
      "Ripple Carry Adder Results:\n",
      "A: [1, 0, 1, 1]\n",
      "B: [1, 1, 0, 1]\n",
      "Sum: [1, 0, 0, 0]\n",
      "Carry-out: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Full Adder Implementation (Reused from previous code)\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        self.w1 = np.random.rand(input_size, hidden_size)\n",
    "        self.b1 = np.random.rand(hidden_size)\n",
    "        self.w2 = np.random.rand(hidden_size, output_size)\n",
    "        self.b2 = np.random.rand(output_size)\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y, output):\n",
    "        output_error = y - output\n",
    "        output_delta = output_error * self.sigmoid_derivative(output)\n",
    "        hidden_error = output_delta.dot(self.w2.T)\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.a1)\n",
    "        \n",
    "        grad_w2 = self.a1.T.dot(output_delta)\n",
    "        grad_b2 = np.sum(output_delta, axis=0)\n",
    "        grad_w1 = X.T.dot(hidden_delta)\n",
    "        grad_b1 = np.sum(hidden_delta, axis=0)\n",
    "        \n",
    "        self.w2 += self.lr * grad_w2\n",
    "        self.b2 += self.lr * grad_b2\n",
    "        self.w1 += self.lr * grad_w1\n",
    "        self.b1 += self.lr * grad_b1\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X)\n",
    "            self.backward(X, y, output)\n",
    "            if epoch % 1000 == 0:\n",
    "                loss = np.mean((y - output) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Full Adder Inputs (For sum and carry-out)\n",
    "X_full_adder = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
    "y_full_adder = np.array([[0, 0], [1, 0], [1, 0], [0, 1], [1, 0], [0, 1], [0, 1], [1, 1]])\n",
    "\n",
    "# Train the XOR gate for Sum calculation\n",
    "sum_nn = NeuralNetwork(input_size=3, hidden_size=3, output_size=1)\n",
    "sum_nn.train(X_full_adder, y_full_adder[:, 0:1], epochs=10000)\n",
    "\n",
    "# Train the Carry-out perceptron (AND-OR) for Carry-out\n",
    "class CarryOutPerceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.random.rand(input_size + 1)  # +1 for bias\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def activation(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        inputs = np.append(inputs, 1)  # Add bias input\n",
    "        weighted_sum = np.dot(inputs, self.weights)\n",
    "        return self.activation(weighted_sum)\n",
    "    \n",
    "    def train(self, X, y, epochs):\n",
    "        for _ in range(epochs):\n",
    "            for inputs, target in zip(X, y):\n",
    "                prediction = self.predict(inputs)\n",
    "                error = target - prediction\n",
    "                inputs = np.append(inputs, 1)  # Add bias input\n",
    "                self.weights += self.lr * error * inputs\n",
    "\n",
    "# Train Carry-out perceptron\n",
    "carry_out_nn = CarryOutPerceptron(input_size=3)\n",
    "carry_out_nn.train(X_full_adder, y_full_adder[:, 1], epochs=10000)\n",
    "\n",
    "# Ripple Carry Adder Implementation\n",
    "def ripple_carry_adder(A, B):\n",
    "    num_bits = len(A)\n",
    "    carry_in = 0\n",
    "    sum_result = []\n",
    "    carry_out_result = []\n",
    "    \n",
    "    for i in range(num_bits):\n",
    "        # Get the Sum bit (using XOR gate)\n",
    "        sum_bit = sum_nn.forward([A[i], B[i], carry_in])\n",
    "        sum_result.append(round(sum_bit[0]))  # Round to nearest integer (0 or 1)\n",
    "\n",
    "        # Get the Carry-out bit (using AND-OR perceptron)\n",
    "        carry_out = carry_out_nn.predict([A[i], B[i], carry_in])\n",
    "        carry_out_result.append(carry_out)\n",
    "        \n",
    "        # Update carry-in for next iteration\n",
    "        carry_in = carry_out\n",
    "    \n",
    "    return sum_result, carry_out_result[-1]  # Sum and final carry-out\n",
    "\n",
    "# Testing Ripple Carry Adder with 4-bit inputs\n",
    "A = [1, 0, 1, 1]  # Example 4-bit input A (11 in decimal)\n",
    "B = [1, 1, 0, 1]  # Example 4-bit input B (13 in decimal)\n",
    "\n",
    "sum_result, carry_out = ripple_carry_adder(A, B)\n",
    "\n",
    "print(\"Ripple Carry Adder Results:\")\n",
    "print(f\"A: {A}\")\n",
    "print(f\"B: {B}\")\n",
    "print(f\"Sum: {sum_result}\")\n",
    "print(f\"Carry-out: {carry_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
