{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4BQVnaAHC4F"
      },
      "source": [
        "## Working with Tensorflow\n",
        "In this assignment, you will be familiarized with the usage of the tensorflow library and how to build a model for the MNIST database in two ways\n",
        "*   using the inbuilt layers in tensorflow\n",
        "*   using custom layers to replicate the same result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Co1Y3oSoAqHp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n682ZxROHh_1"
      },
      "source": [
        "## Loading and preprocessing the Data\n",
        "We will directly be using the dataset included in tensorflow library\n",
        "A detailed description of data is given at (https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8zrkUXY8AvtN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NZRlpvMIDxs"
      },
      "source": [
        "Heres how the data looks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "9jFXtFL4FH2x",
        "outputId": "8b9450bc-0e10-4776-a8a1-1dea5cac909d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x22ead691ee0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(x_train[0],cmap= 'Greys')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2loSsKCGScS",
        "outputId": "8b6ec3ab-0458-4175-903d-37619b48b848"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "-VO0bDzWGN0z",
        "outputId": "4ac2e0d4-a1ce-4a49-cfcc-ce474d8b3d1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x22ead6fd2e0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(x_train[1],cmap= 'Greys')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "WmwFY7-9GP_-",
        "outputId": "0ae5cd81-0404-4b53-a1cb-c3459d7edd8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x22eae086db0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(x_train[2],cmap= 'Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6W4VEerGYJ8"
      },
      "source": [
        "# Making a simple feedforward network\n",
        "As you have seen in the second week a simple feedfordward network works well to solve MNIST.<br/>\n",
        "The following is a simple feedforward model with three layers:\n",
        "* a flatten layer to convert our 28x28 images into a single array of length 784\n",
        "* a dense layer of 128 neurons with the relu activation function\n",
        "* finally a dense layer of 10 neurons with the softmax activation to get a distribution between the digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwFnJsE6vjf8",
        "outputId": "ac06dbd4-9873-4366-d212-8d784b6eab77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.8610 - loss: 0.4916 - val_accuracy: 0.9546 - val_loss: 0.1622\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9574 - loss: 0.1476 - val_accuracy: 0.9642 - val_loss: 0.1213\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9718 - loss: 0.0944 - val_accuracy: 0.9700 - val_loss: 0.1009\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9797 - loss: 0.0654 - val_accuracy: 0.9757 - val_loss: 0.0855\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.9856 - loss: 0.0482 - val_accuracy: 0.9740 - val_loss: 0.0859\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9733 - loss: 0.0871\n",
            "Test accuracy: 0.9771999716758728\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7yTvXD_JBq2"
      },
      "source": [
        "## Making Custom Layers\n",
        "You can go through this\n",
        "<a href = 'https://www.tensorflow.org/tutorials/customization/custom_layers'> documentation </a> to get a feel for how to implement a custom layer\n",
        "\n",
        "* Create a CustomDenseLayer with a Relu Activation\n",
        "* Create a CustomDenseLayer with a Softmax Activation\n",
        "* Create a CustomFlatten Layer\n",
        "\n",
        "Altough we have provided solutions in the Next Cell Try to figure this out on your own.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7wtMoyyNK-FZ"
      },
      "outputs": [],
      "source": [
        "class CustomDenseReluLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(CustomDenseReluLayer, self).__init__()\n",
        "        #TODO\n",
        "        self.units=units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        #TODO\n",
        "        self.w=self.add_weight(shape=(input_shape[-1],self.units),initializer='random_normal',trainable=True)\n",
        "        self.b=self.add_weight(shape=(self.units,),initializer='zeros',trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #TODO\n",
        "        z=tf.matmul(inputs,self.w)+self.b\n",
        "        z=tf.nn.relu(z)\n",
        "        return z\n",
        "\n",
        "class CustomDenseSoftmaxLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(CustomDenseSoftmaxLayer, self).__init__()\n",
        "        #TODO\n",
        "        self.units=units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        #TODO\n",
        "        self.w=self.add_weight(shape=(input_shape[-1],self.units),initializer=\"random_normal\",trainable=True)\n",
        "        self.b=self.add_weight(shape=(self.units,),initializer='zeros',trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #TODO\n",
        "        z=tf.matmul(inputs,self.w)+self.b\n",
        "        z=tf.nn.softmax(z)\n",
        "        return z\n",
        "    \n",
        "class CustomFlattenLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        #TODO\n",
        "        \n",
        "        return tf.reshape(inputs, (tf.shape(inputs)[0],-1))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCEMWK1KLZDT"
      },
      "source": [
        "## Using out custom layers to Build a model for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zw53cAmELYJK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.8435 - loss: 0.5537 - val_accuracy: 0.9516 - val_loss: 0.1676\n",
            "Epoch 2/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9554 - loss: 0.1521 - val_accuracy: 0.9588 - val_loss: 0.1391\n",
            "Epoch 3/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9715 - loss: 0.0961 - val_accuracy: 0.9685 - val_loss: 0.1026\n",
            "Epoch 4/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9801 - loss: 0.0703 - val_accuracy: 0.9697 - val_loss: 0.1000\n",
            "Epoch 5/5\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9841 - loss: 0.0537 - val_accuracy: 0.9721 - val_loss: 0.0941\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.1049\n",
            "Test accuracy: 0.9728999733924866\n"
          ]
        }
      ],
      "source": [
        "# Example usage of the custom dense layer\n",
        "model = Sequential([\n",
        "    CustomFlattenLayer(),\n",
        "    CustomDenseReluLayer(128),\n",
        "    CustomDenseSoftmaxLayer(10)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ymh-5LLyMY"
      },
      "source": [
        "# Assignment\n",
        "* Load and preprocess Boston housing dataset\n",
        "* build a Linear Regression model for it and optimize it using tensorflow (its basically a neural network with a single neuron and no activaton)\n",
        "* build a Feedforward network for it you can expirement around with no of layers and and neurons in each layer and different activation functions <br/>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
              "          3.96900e+02, 1.87200e+01],\n",
              "         [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
              "          3.95380e+02, 3.11000e+00],\n",
              "         [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
              "          3.75520e+02, 3.26000e+00],\n",
              "         ...,\n",
              "         [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
              "          3.62250e+02, 7.83000e+00],\n",
              "         [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
              "          2.61950e+02, 1.57900e+01],\n",
              "         [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
              "          3.76700e+02, 4.38000e+00]]),\n",
              "  array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
              "         17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
              "         32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
              "         23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
              "         12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
              "         22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
              "         15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
              "         14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
              "         14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
              "         28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
              "         19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
              "         18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
              "         31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
              "         19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
              "         22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
              "         27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
              "          8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
              "         19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
              "         23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
              "         21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
              "         17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
              "         16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
              "         24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
              "         13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
              "         22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
              "         23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
              "          7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
              "          8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
              "         19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
              "         19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
              "         23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
              "         19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
              "         23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
              "         33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
              "         28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
              "         24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
              "         11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])),\n",
              " (array([[1.80846e+01, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
              "          2.72500e+01, 2.90500e+01],\n",
              "         [1.23290e-01, 0.00000e+00, 1.00100e+01, ..., 1.78000e+01,\n",
              "          3.94950e+02, 1.62100e+01],\n",
              "         [5.49700e-02, 0.00000e+00, 5.19000e+00, ..., 2.02000e+01,\n",
              "          3.96900e+02, 9.74000e+00],\n",
              "         ...,\n",
              "         [1.83377e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
              "          3.89610e+02, 1.92000e+00],\n",
              "         [3.58090e-01, 0.00000e+00, 6.20000e+00, ..., 1.74000e+01,\n",
              "          3.91700e+02, 9.71000e+00],\n",
              "         [2.92400e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
              "          2.40160e+02, 9.81000e+00]]),\n",
              "  array([ 7.2, 18.8, 19. , 27. , 22.2, 24.5, 31.2, 22.9, 20.5, 23.2, 18.6,\n",
              "         14.5, 17.8, 50. , 20.8, 24.3, 24.2, 19.8, 19.1, 22.7, 12. , 10.2,\n",
              "         20. , 18.5, 20.9, 23. , 27.5, 30.1,  9.5, 22. , 21.2, 14.1, 33.1,\n",
              "         23.4, 20.1,  7.4, 15.4, 23.8, 20.1, 24.5, 33. , 28.4, 14.1, 46.7,\n",
              "         32.5, 29.6, 28.4, 19.8, 20.2, 25. , 35.4, 20.3,  9.7, 14.5, 34.9,\n",
              "         26.6,  7.2, 50. , 32.4, 21.6, 29.8, 13.1, 27.5, 21.2, 23.1, 21.9,\n",
              "         13. , 23.2,  8.1,  5.6, 21.7, 29.6, 19.6,  7. , 26.4, 18.9, 20.9,\n",
              "         28.1, 35.4, 10.2, 24.3, 43.1, 17.6, 15.4, 16.2, 27.1, 21.4, 21.5,\n",
              "         22.4, 25. , 16.6, 18.6, 22. , 42.8, 35.1, 21.5, 36. , 21.9, 24.1,\n",
              "         50. , 26.7, 25. ])))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "boston_data=boston_housing.load_data()\n",
        "boston_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = boston_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (404, 13), Training labels shape: (404,)\n",
            "Testing data shape: (102, 13), Testing labels shape: (102,)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training data shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}, Testing labels shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear Regression Model (Single Neuron)\n",
        "def build_linear_reg_model(input_shape):\n",
        "    model=tf.keras.Sequential([\n",
        "        \n",
        "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(1, activation=None)\n",
        "        \n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 14951.1963 - val_loss: 2163.7405\n",
            "Epoch 2/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1147.6636 - val_loss: 1045.0972\n",
            "Epoch 3/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1142.5391 - val_loss: 545.1927\n",
            "Epoch 4/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 510.0278 - val_loss: 505.8585\n",
            "Epoch 5/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 418.1589 - val_loss: 372.2034\n",
            "Epoch 6/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 311.8089 - val_loss: 310.1593\n",
            "Epoch 7/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 272.7337 - val_loss: 291.6060\n",
            "Epoch 8/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 251.0942 - val_loss: 276.3362\n",
            "Epoch 9/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 235.9665 - val_loss: 253.0440\n",
            "Epoch 10/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 228.3557 - val_loss: 244.6557\n",
            "Epoch 11/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 215.1853 - val_loss: 230.8469\n",
            "Epoch 12/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 184.2710 - val_loss: 224.8508\n",
            "Epoch 13/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 173.0785 - val_loss: 205.7289\n",
            "Epoch 14/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 174.9649 - val_loss: 202.5496\n",
            "Epoch 15/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 159.4727 - val_loss: 189.8650\n",
            "Epoch 16/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 168.5483 - val_loss: 180.3854\n",
            "Epoch 17/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - loss: 151.0944 - val_loss: 170.1921\n",
            "Epoch 18/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 142.3356 - val_loss: 164.4773\n",
            "Epoch 19/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 143.3888 - val_loss: 150.9233\n",
            "Epoch 20/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 140.7608 - val_loss: 148.3526\n",
            "Epoch 21/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 130.8280 - val_loss: 137.4366\n",
            "Epoch 22/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 127.0909 - val_loss: 130.9555\n",
            "Epoch 23/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 103.0857 - val_loss: 125.4073\n",
            "Epoch 24/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 101.6221 - val_loss: 116.6416\n",
            "Epoch 25/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 101.3143 - val_loss: 112.7913\n",
            "Epoch 26/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 94.5348 - val_loss: 107.0361\n",
            "Epoch 27/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 90.0008 - val_loss: 101.6616\n",
            "Epoch 28/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 83.4894 - val_loss: 96.5535\n",
            "Epoch 29/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 83.0116 - val_loss: 91.7441\n",
            "Epoch 30/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 76.9951 - val_loss: 87.4665\n",
            "Epoch 31/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 69.3739 - val_loss: 84.4211\n",
            "Epoch 32/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 77.1761 - val_loss: 80.7037\n",
            "Epoch 33/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 62.7015 - val_loss: 77.8820\n",
            "Epoch 34/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 66.2356 - val_loss: 74.1358\n",
            "Epoch 35/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 68.4143 - val_loss: 73.3882\n",
            "Epoch 36/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 64.6289 - val_loss: 69.1651\n",
            "Epoch 37/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 61.2670 - val_loss: 67.4389\n",
            "Epoch 38/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 65.7770 - val_loss: 65.4701\n",
            "Epoch 39/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 52.2733 - val_loss: 63.4475\n",
            "Epoch 40/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 59.1688 - val_loss: 61.5571\n",
            "Epoch 41/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 54.3325 - val_loss: 60.1813\n",
            "Epoch 42/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 52.0547 - val_loss: 58.7857\n",
            "Epoch 43/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 50.8897 - val_loss: 58.1664\n",
            "Epoch 44/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 52.6583 - val_loss: 56.2518\n",
            "Epoch 45/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 52.9222 - val_loss: 56.2119\n",
            "Epoch 46/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 51.3393 - val_loss: 54.4520\n",
            "Epoch 47/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 56.2552 - val_loss: 54.9651\n",
            "Epoch 48/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 46.7614 - val_loss: 52.9746\n",
            "Epoch 49/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 45.4130 - val_loss: 53.4964\n",
            "Epoch 50/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 49.5086 - val_loss: 51.7632\n",
            "Epoch 51/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 47.9180 - val_loss: 51.2790\n",
            "Epoch 52/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 48.6717 - val_loss: 50.8359\n",
            "Epoch 53/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 44.4984 - val_loss: 49.8402\n",
            "Epoch 54/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 45.8171 - val_loss: 49.7435\n",
            "Epoch 55/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 48.0532 - val_loss: 49.3282\n",
            "Epoch 56/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 45.7074 - val_loss: 49.8107\n",
            "Epoch 57/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 50.5359 - val_loss: 48.5405\n",
            "Epoch 58/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 44.8705 - val_loss: 48.3753\n",
            "Epoch 59/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 42.5409 - val_loss: 48.7832\n",
            "Epoch 60/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 47.5334 - val_loss: 47.9250\n",
            "Epoch 61/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 46.4128 - val_loss: 47.8130\n",
            "Epoch 62/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 43.0960 - val_loss: 47.1150\n",
            "Epoch 63/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 43.4612 - val_loss: 47.5404\n",
            "Epoch 64/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 47.5826 - val_loss: 47.1127\n",
            "Epoch 65/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 38.9246 - val_loss: 46.9469\n",
            "Epoch 66/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 40.6273 - val_loss: 46.3594\n",
            "Epoch 67/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 44.8006 - val_loss: 46.1187\n",
            "Epoch 68/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 48.8520 - val_loss: 45.9480\n",
            "Epoch 69/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 41.4255 - val_loss: 46.0758\n",
            "Epoch 70/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 46.1081 - val_loss: 46.2620\n",
            "Epoch 71/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 40.1556 - val_loss: 46.0699\n",
            "Epoch 72/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 40.4488 - val_loss: 45.8303\n",
            "Epoch 73/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 45.7502 - val_loss: 45.0280\n",
            "Epoch 74/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 38.1425 - val_loss: 45.3201\n",
            "Epoch 75/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39.6459 - val_loss: 44.8634\n",
            "Epoch 76/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 38.4882 - val_loss: 46.8961\n",
            "Epoch 77/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 40.0295 - val_loss: 44.9898\n",
            "Epoch 78/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 44.2555 - val_loss: 44.6759\n",
            "Epoch 79/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 38.6582 - val_loss: 45.1673\n",
            "Epoch 80/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 41.3597 - val_loss: 45.2385\n",
            "Epoch 81/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 40.4613 - val_loss: 44.2000\n",
            "Epoch 82/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 44.2369 - val_loss: 45.3434\n",
            "Epoch 83/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 41.0700 - val_loss: 46.3328\n",
            "Epoch 84/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 38.3398 - val_loss: 44.1168\n",
            "Epoch 85/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 40.2250 - val_loss: 44.0273\n",
            "Epoch 86/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 37.1003 - val_loss: 43.5458\n",
            "Epoch 87/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 35.7838 - val_loss: 46.3635\n",
            "Epoch 88/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 39.8082 - val_loss: 43.8093\n",
            "Epoch 89/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 43.1803 - val_loss: 43.7075\n",
            "Epoch 90/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 40.5061 - val_loss: 43.2089\n",
            "Epoch 91/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 39.7669 - val_loss: 43.1604\n",
            "Epoch 92/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 36.2919 - val_loss: 43.3077\n",
            "Epoch 93/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 39.1063 - val_loss: 43.1453\n",
            "Epoch 94/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 39.8807 - val_loss: 42.8292\n",
            "Epoch 95/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 39.0334 - val_loss: 42.6995\n",
            "Epoch 96/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 39.5066 - val_loss: 43.1893\n",
            "Epoch 97/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 40.4268 - val_loss: 42.5482\n",
            "Epoch 98/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 35.9455 - val_loss: 43.2382\n",
            "Epoch 99/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 41.3104 - val_loss: 42.4461\n",
            "Epoch 100/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 38.2118 - val_loss: 43.1260\n",
            "Epoch 101/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 42.5976 - val_loss: 42.1655\n",
            "Epoch 102/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 37.4820 - val_loss: 42.2058\n",
            "Epoch 103/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 38.0227 - val_loss: 42.7106\n",
            "Epoch 104/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 44.5124 - val_loss: 44.5805\n",
            "Epoch 105/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 39.7348 - val_loss: 43.6865\n",
            "Epoch 106/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 36.8362 - val_loss: 42.8552\n",
            "Epoch 107/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 36.0682 - val_loss: 41.7854\n",
            "Epoch 108/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 38.2757 - val_loss: 41.7043\n",
            "Epoch 109/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 40.6826 - val_loss: 42.5774\n",
            "Epoch 110/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 36.5817 - val_loss: 41.8097\n",
            "Epoch 111/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 34.0992 - val_loss: 42.3758\n",
            "Epoch 112/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 34.3701 - val_loss: 41.6979\n",
            "Epoch 113/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 34.4469 - val_loss: 41.4308\n",
            "Epoch 114/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 34.2505 - val_loss: 45.4304\n",
            "Epoch 115/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 40.1187 - val_loss: 41.6459\n",
            "Epoch 116/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 38.1690 - val_loss: 41.5215\n",
            "Epoch 117/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 33.9417 - val_loss: 41.0612\n",
            "Epoch 118/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 35.1433 - val_loss: 41.1094\n",
            "Epoch 119/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 39.7662 - val_loss: 41.8179\n",
            "Epoch 120/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 38.0072 - val_loss: 41.3572\n",
            "Epoch 121/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 38.6687 - val_loss: 44.0345\n",
            "Epoch 122/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 36.9898 - val_loss: 42.8585\n",
            "Epoch 123/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 44.4041 - val_loss: 41.0120\n",
            "Epoch 124/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 34.3826 - val_loss: 40.2799\n",
            "Epoch 125/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 33.5226 - val_loss: 44.1944\n",
            "Epoch 126/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 41.3187 - val_loss: 40.0593\n",
            "Epoch 127/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 44.5129 - val_loss: 40.3264\n",
            "Epoch 128/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 39.8317 - val_loss: 40.3809\n",
            "Epoch 129/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 38.5632 - val_loss: 40.6873\n",
            "Epoch 130/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 33.7228 - val_loss: 40.7092\n",
            "Epoch 131/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 40.0128 - val_loss: 40.0690\n",
            "Epoch 132/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 36.8493 - val_loss: 43.3450\n",
            "Epoch 133/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 32.6114 - val_loss: 39.8185\n",
            "Epoch 134/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 32.6443 - val_loss: 40.4157\n",
            "Epoch 135/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 36.6840 - val_loss: 39.9024\n",
            "Epoch 136/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 40.1932 - val_loss: 40.0416\n",
            "Epoch 137/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 32.9488 - val_loss: 39.6590\n",
            "Epoch 138/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 35.6597 - val_loss: 40.0881\n",
            "Epoch 139/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 35.6831 - val_loss: 39.5177\n",
            "Epoch 140/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 35.9414 - val_loss: 41.6492\n",
            "Epoch 141/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 36.6043 - val_loss: 39.4224\n",
            "Epoch 142/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 32.1925 - val_loss: 39.7501\n",
            "Epoch 143/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 36.6063 - val_loss: 39.5310\n",
            "Epoch 144/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 33.3392 - val_loss: 39.2188\n",
            "Epoch 145/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 32.2635 - val_loss: 41.6192\n",
            "Epoch 146/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 32.5582 - val_loss: 39.9001\n",
            "Epoch 147/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 36.7276 - val_loss: 40.7851\n",
            "Epoch 148/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 37.9460 - val_loss: 41.5124\n",
            "Epoch 149/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 36.4186 - val_loss: 39.9140\n",
            "Epoch 150/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 36.2905 - val_loss: 38.7667\n",
            "Epoch 151/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 31.0934 - val_loss: 38.4843\n",
            "Epoch 152/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 38.7523 - val_loss: 41.2889\n",
            "Epoch 153/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 37.0356 - val_loss: 41.6445\n",
            "Epoch 154/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 31.2739 - val_loss: 41.3487\n",
            "Epoch 155/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 37.8995 - val_loss: 39.5517\n",
            "Epoch 156/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 41.6730 - val_loss: 43.6127\n",
            "Epoch 157/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 37.3556 - val_loss: 42.6535\n",
            "Epoch 158/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 32.6372 - val_loss: 38.0876\n",
            "Epoch 159/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 32.6052 - val_loss: 38.7546\n",
            "Epoch 160/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 36.5624 - val_loss: 38.6414\n",
            "Epoch 161/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 33.9515 - val_loss: 38.7483\n",
            "Epoch 162/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 36.2426 - val_loss: 41.4614\n",
            "Epoch 163/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 37.8092 - val_loss: 39.4705\n",
            "Epoch 164/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 36.1483 - val_loss: 37.8394\n",
            "Epoch 165/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 35.5033 - val_loss: 38.0861\n",
            "Epoch 166/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 35.3946 - val_loss: 41.5429\n",
            "Epoch 167/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 29.1507 - val_loss: 43.5093\n",
            "Epoch 168/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 34.2134 - val_loss: 37.6739\n",
            "Epoch 169/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 35.5465 - val_loss: 36.9497\n",
            "Epoch 170/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 33.8777 - val_loss: 37.0524\n",
            "Epoch 171/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 33.4881 - val_loss: 37.6371\n",
            "Epoch 172/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 31.7028 - val_loss: 39.7789\n",
            "Epoch 173/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 38.2249 - val_loss: 38.2336\n",
            "Epoch 174/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 30.7468 - val_loss: 38.8847\n",
            "Epoch 175/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 34.0340 - val_loss: 38.0070\n",
            "Epoch 176/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 30.7662 - val_loss: 36.8809\n",
            "Epoch 177/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 31.9872 - val_loss: 37.9619\n",
            "Epoch 178/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 45.2045 - val_loss: 37.5356\n",
            "Epoch 179/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 36.8315 - val_loss: 37.0683\n",
            "Epoch 180/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 30.0289 - val_loss: 40.4250\n",
            "Epoch 181/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 36.2237 - val_loss: 39.4314\n",
            "Epoch 182/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 37.3538 - val_loss: 36.8970\n",
            "Epoch 183/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 35.5322 - val_loss: 36.2172\n",
            "Epoch 184/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 31.4593 - val_loss: 37.5871\n",
            "Epoch 185/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 32.7893 - val_loss: 37.7307\n",
            "Epoch 186/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 37.6913 - val_loss: 38.6677\n",
            "Epoch 187/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 30.8766 - val_loss: 36.3992\n",
            "Epoch 188/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 37.1043 - val_loss: 37.7226\n",
            "Epoch 189/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 37.3583 - val_loss: 36.8165\n",
            "Epoch 190/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 32.6107 - val_loss: 35.8099\n",
            "Epoch 191/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 29.9348 - val_loss: 36.3249\n",
            "Epoch 192/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 33.3546 - val_loss: 36.3989\n",
            "Epoch 193/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 34.8502 - val_loss: 36.2881\n",
            "Epoch 194/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 39.7843 - val_loss: 36.1391\n",
            "Epoch 195/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 38.7635 - val_loss: 36.8662\n",
            "Epoch 196/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 32.5464 - val_loss: 37.9159\n",
            "Epoch 197/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 33.4111 - val_loss: 36.3715\n",
            "Epoch 198/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 32.9629 - val_loss: 37.6273\n",
            "Epoch 199/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 30.5496 - val_loss: 36.7006\n",
            "Epoch 200/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 32.9128 - val_loss: 36.1604\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 31.6678\n",
            "Linear Regression Model Mean Squared Error: 36.16035079956055\n"
          ]
        }
      ],
      "source": [
        "model_linear=build_linear_reg_model(X_train.shape[1:])\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "loss=\"mean_squared_error\"\n",
        "model_linear.compile(optimizer,loss)\n",
        "history_linear=model_linear.fit(X_train,y_train,epochs=200,batch_size=32,validation_data=(X_test,y_test))\n",
        "mse_linear=model_linear.evaluate(X_test,y_test)\n",
        "print(f\"Linear Regression Model Mean Squared Error: {mse_linear}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: 2861.6169 - val_loss: 263.3358\n",
            "Epoch 2/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 186.2282 - val_loss: 85.5224\n",
            "Epoch 3/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 113.3437 - val_loss: 78.4376\n",
            "Epoch 4/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 100.7558 - val_loss: 74.4733\n",
            "Epoch 5/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 71.7874 - val_loss: 71.7188\n",
            "Epoch 6/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 76.1674 - val_loss: 69.4041\n",
            "Epoch 7/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 64.5734 - val_loss: 65.0826\n",
            "Epoch 8/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 65.5758 - val_loss: 62.9922\n",
            "Epoch 9/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 74.6689 - val_loss: 62.2567\n",
            "Epoch 10/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 59.5468 - val_loss: 60.3776\n",
            "Epoch 11/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 56.5516 - val_loss: 63.7813\n",
            "Epoch 12/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 59.1277 - val_loss: 59.8926\n",
            "Epoch 13/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 67.0832 - val_loss: 61.8539\n",
            "Epoch 14/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 60.1029 - val_loss: 64.1729\n",
            "Epoch 15/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 53.5976 - val_loss: 58.5887\n",
            "Epoch 16/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 65.3923 - val_loss: 58.0316\n",
            "Epoch 17/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 54.8320 - val_loss: 56.7631\n",
            "Epoch 18/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 46.0884 - val_loss: 58.6159\n",
            "Epoch 19/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 48.0249 - val_loss: 56.7707\n",
            "Epoch 20/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 43.6545 - val_loss: 56.6320\n",
            "Epoch 21/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 49.1887 - val_loss: 57.0723\n",
            "Epoch 22/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 55.4224 - val_loss: 54.9433\n",
            "Epoch 23/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 58.1117 - val_loss: 57.4102\n",
            "Epoch 24/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 46.9603 - val_loss: 52.7179\n",
            "Epoch 25/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 40.1451 - val_loss: 52.7152\n",
            "Epoch 26/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 47.2191 - val_loss: 52.4705\n",
            "Epoch 27/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 45.2065 - val_loss: 52.3881\n",
            "Epoch 28/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 37.3071 - val_loss: 52.8276\n",
            "Epoch 29/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 48.6480 - val_loss: 49.2011\n",
            "Epoch 30/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 43.8254 - val_loss: 49.3047\n",
            "Epoch 31/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 34.7982 - val_loss: 49.2945\n",
            "Epoch 32/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 43.8283 - val_loss: 44.9735\n",
            "Epoch 33/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 38.4871 - val_loss: 46.4026\n",
            "Epoch 34/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 42.1910 - val_loss: 45.1203\n",
            "Epoch 35/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 37.4708 - val_loss: 42.9399\n",
            "Epoch 36/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 43.1468 - val_loss: 41.6000\n",
            "Epoch 37/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 35.5380 - val_loss: 41.5740\n",
            "Epoch 38/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 42.6737 - val_loss: 50.2613\n",
            "Epoch 39/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 34.2487 - val_loss: 42.4598\n",
            "Epoch 40/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 39.8925 - val_loss: 38.4945\n",
            "Epoch 41/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 36.3348 - val_loss: 42.7035\n",
            "Epoch 42/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 33.9881 - val_loss: 45.9534\n",
            "Epoch 43/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 35.6047 - val_loss: 45.0810\n",
            "Epoch 44/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 38.6698 - val_loss: 49.9564\n",
            "Epoch 45/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 32.6547 - val_loss: 55.1720\n",
            "Epoch 46/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 42.8156 - val_loss: 57.7923\n",
            "Epoch 47/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 47.7279 - val_loss: 36.9783\n",
            "Epoch 48/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 36.7072 - val_loss: 34.8891\n",
            "Epoch 49/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 34.7754 - val_loss: 35.8155\n",
            "Epoch 50/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 30.2638 - val_loss: 38.8700\n",
            "Epoch 51/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 29.2194 - val_loss: 36.6980\n",
            "Epoch 52/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 34.7587 - val_loss: 35.8736\n",
            "Epoch 53/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 27.2806 - val_loss: 34.5969\n",
            "Epoch 54/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 32.0874 - val_loss: 37.2001\n",
            "Epoch 55/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 29.4654 - val_loss: 34.5043\n",
            "Epoch 56/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 26.7999 - val_loss: 34.1344\n",
            "Epoch 57/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 36.4205 - val_loss: 32.2788\n",
            "Epoch 58/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 33.2298 - val_loss: 34.3279\n",
            "Epoch 59/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 35.4316 - val_loss: 32.4431\n",
            "Epoch 60/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 32.2375 - val_loss: 35.1019\n",
            "Epoch 61/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 32.5549 - val_loss: 34.5092\n",
            "Epoch 62/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 27.6036 - val_loss: 31.3472\n",
            "Epoch 63/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 28.5725 - val_loss: 34.6143\n",
            "Epoch 64/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 34.9988 - val_loss: 40.0240\n",
            "Epoch 65/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 31.1723 - val_loss: 32.9425\n",
            "Epoch 66/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 28.8448 - val_loss: 30.4983\n",
            "Epoch 67/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 32.9477 - val_loss: 43.4837\n",
            "Epoch 68/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 38.6818 - val_loss: 31.1271\n",
            "Epoch 69/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 32.0111 - val_loss: 30.1277\n",
            "Epoch 70/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 31.3747 - val_loss: 37.7659\n",
            "Epoch 71/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 37.1065 - val_loss: 29.8101\n",
            "Epoch 72/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 26.4806 - val_loss: 29.8732\n",
            "Epoch 73/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 32.7788 - val_loss: 31.9916\n",
            "Epoch 74/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 27.3739 - val_loss: 29.9349\n",
            "Epoch 75/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 26.3458 - val_loss: 29.2208\n",
            "Epoch 76/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 26.5088 - val_loss: 29.6515\n",
            "Epoch 77/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 29.6272 - val_loss: 29.2535\n",
            "Epoch 78/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 26.2214 - val_loss: 30.6151\n",
            "Epoch 79/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 27.7822 - val_loss: 28.9468\n",
            "Epoch 80/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 23.9046 - val_loss: 32.4494\n",
            "Epoch 81/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 22.6585 - val_loss: 54.7318\n",
            "Epoch 82/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 37.5807 - val_loss: 28.3738\n",
            "Epoch 83/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 28.7983 - val_loss: 30.6512\n",
            "Epoch 84/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 28.6626 - val_loss: 29.5740\n",
            "Epoch 85/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 26.3238 - val_loss: 31.5161\n",
            "Epoch 86/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 27.2185 - val_loss: 31.4654\n",
            "Epoch 87/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.9943 - val_loss: 38.2521\n",
            "Epoch 88/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 26.6982 - val_loss: 28.1517\n",
            "Epoch 89/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 22.7661 - val_loss: 29.9180\n",
            "Epoch 90/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 20.2123 - val_loss: 36.4161\n",
            "Epoch 91/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 28.1500 - val_loss: 27.5953\n",
            "Epoch 92/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 27.5219 - val_loss: 30.9329\n",
            "Epoch 93/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 24.8126 - val_loss: 29.3235\n",
            "Epoch 94/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 25.3462 - val_loss: 28.8503\n",
            "Epoch 95/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 22.6728 - val_loss: 29.7289\n",
            "Epoch 96/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 25.1886 - val_loss: 35.0554\n",
            "Epoch 97/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 26.6710 - val_loss: 31.3021\n",
            "Epoch 98/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 25.1410 - val_loss: 31.3326\n",
            "Epoch 99/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 24.0999 - val_loss: 30.5555\n",
            "Epoch 100/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 25.4321 - val_loss: 31.8911\n",
            "Epoch 101/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.8733 - val_loss: 29.0873\n",
            "Epoch 102/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 20.4514 - val_loss: 28.2988\n",
            "Epoch 103/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 21.4111 - val_loss: 26.4171\n",
            "Epoch 104/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 20.8429 - val_loss: 27.6786\n",
            "Epoch 105/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 21.4595 - val_loss: 27.4350\n",
            "Epoch 106/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 22.7579 - val_loss: 28.6790\n",
            "Epoch 107/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 20.5215 - val_loss: 26.9151\n",
            "Epoch 108/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 23.8878 - val_loss: 29.2687\n",
            "Epoch 109/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 24.5710 - val_loss: 28.8938\n",
            "Epoch 110/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 18.9688 - val_loss: 26.6628\n",
            "Epoch 111/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.1682 - val_loss: 31.5250\n",
            "Epoch 112/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 20.0940 - val_loss: 31.1715\n",
            "Epoch 113/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 18.0734 - val_loss: 32.6056\n",
            "Epoch 114/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 22.4619 - val_loss: 26.2575\n",
            "Epoch 115/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 19.8195 - val_loss: 26.2240\n",
            "Epoch 116/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15.9713 - val_loss: 34.4818\n",
            "Epoch 117/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 21.2572 - val_loss: 25.8384\n",
            "Epoch 118/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15.3179 - val_loss: 31.5442\n",
            "Epoch 119/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.2295 - val_loss: 33.7808\n",
            "Epoch 120/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 27.8988 - val_loss: 26.4199\n",
            "Epoch 121/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.6449 - val_loss: 28.7692\n",
            "Epoch 122/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.6272 - val_loss: 34.3752\n",
            "Epoch 123/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 21.9718 - val_loss: 28.3659\n",
            "Epoch 124/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 21.9967 - val_loss: 28.7722\n",
            "Epoch 125/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 16.0169 - val_loss: 33.9825\n",
            "Epoch 126/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 23.3040 - val_loss: 30.8918\n",
            "Epoch 127/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 22.0060 - val_loss: 27.7528\n",
            "Epoch 128/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 17.6712 - val_loss: 25.1277\n",
            "Epoch 129/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 17.8515 - val_loss: 24.3470\n",
            "Epoch 130/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 21.9310 - val_loss: 28.5301\n",
            "Epoch 131/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.2054 - val_loss: 27.7003\n",
            "Epoch 132/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 15.5934 - val_loss: 34.5421\n",
            "Epoch 133/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.1001 - val_loss: 43.0713\n",
            "Epoch 134/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 25.4246 - val_loss: 33.2189\n",
            "Epoch 135/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 29.5307 - val_loss: 36.9350\n",
            "Epoch 136/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 20.8819 - val_loss: 30.1543\n",
            "Epoch 137/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 16.1958 - val_loss: 32.6206\n",
            "Epoch 138/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 20.0588 - val_loss: 27.4937\n",
            "Epoch 139/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16.2514 - val_loss: 29.4814\n",
            "Epoch 140/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 14.9037 - val_loss: 27.6461\n",
            "Epoch 141/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 14.3920 - val_loss: 25.1090\n",
            "Epoch 142/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.1991 - val_loss: 29.2218\n",
            "Epoch 143/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16.8415 - val_loss: 26.7227\n",
            "Epoch 144/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 17.0758 - val_loss: 29.4004\n",
            "Epoch 145/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 19.6169 - val_loss: 36.5839\n",
            "Epoch 146/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 19.3876 - val_loss: 26.5310\n",
            "Epoch 147/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15.4187 - val_loss: 27.1435\n",
            "Epoch 148/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 14.5728 - val_loss: 26.8586\n",
            "Epoch 149/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16.3096 - val_loss: 31.3326\n",
            "Epoch 150/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16.9476 - val_loss: 25.9585\n",
            "Epoch 151/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.6039 - val_loss: 27.5923\n",
            "Epoch 152/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 13.8621 - val_loss: 27.3514\n",
            "Epoch 153/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15.1250 - val_loss: 24.9944\n",
            "Epoch 154/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 15.8044 - val_loss: 36.1910\n",
            "Epoch 155/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.5088 - val_loss: 30.1687\n",
            "Epoch 156/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 14.5673 - val_loss: 30.3682\n",
            "Epoch 157/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 14.0658 - val_loss: 29.3061\n",
            "Epoch 158/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 13.5874 - val_loss: 29.4295\n",
            "Epoch 159/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 15.0821 - val_loss: 29.6907\n",
            "Epoch 160/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 14.6975 - val_loss: 26.7865\n",
            "Epoch 161/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 16.5489 - val_loss: 32.2452\n",
            "Epoch 162/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 17.7827 - val_loss: 25.7055\n",
            "Epoch 163/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 11.1539 - val_loss: 32.3265\n",
            "Epoch 164/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15.1887 - val_loss: 26.3472\n",
            "Epoch 165/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 11.6185 - val_loss: 29.5871\n",
            "Epoch 166/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 15.3639 - val_loss: 29.2294\n",
            "Epoch 167/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 16.5504 - val_loss: 34.6621\n",
            "Epoch 168/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 15.7789 - val_loss: 30.7424\n",
            "Epoch 169/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 16.1030 - val_loss: 27.2920\n",
            "Epoch 170/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 14.8374 - val_loss: 36.5057\n",
            "Epoch 171/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 20.2829 - val_loss: 33.5609\n",
            "Epoch 172/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 24.0614 - val_loss: 24.5305\n",
            "Epoch 173/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 15.0862 - val_loss: 28.8502\n",
            "Epoch 174/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 15.1178 - val_loss: 31.8496\n",
            "Epoch 175/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 19.2844 - val_loss: 31.1967\n",
            "Epoch 176/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 14.2119 - val_loss: 44.7759\n",
            "Epoch 177/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19.2526 - val_loss: 30.2564\n",
            "Epoch 178/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.3634 - val_loss: 28.8334\n",
            "Epoch 179/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 13.3163 - val_loss: 26.8291\n",
            "Epoch 180/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 16.4676 - val_loss: 27.8348\n",
            "Epoch 181/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 13.9499 - val_loss: 31.5118\n",
            "Epoch 182/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 19.2013 - val_loss: 55.6926\n",
            "Epoch 183/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 25.6863 - val_loss: 26.5667\n",
            "Epoch 184/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 15.4583 - val_loss: 26.4402\n",
            "Epoch 185/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 12.1716 - val_loss: 28.2375\n",
            "Epoch 186/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 11.3653 - val_loss: 27.0038\n",
            "Epoch 187/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 12.4711 - val_loss: 30.6687\n",
            "Epoch 188/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 15.5409 - val_loss: 26.4729\n",
            "Epoch 189/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 14.1334 - val_loss: 31.7975\n",
            "Epoch 190/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13.7741 - val_loss: 27.0887\n",
            "Epoch 191/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 13.2930 - val_loss: 26.7951\n",
            "Epoch 192/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.3130 - val_loss: 31.0562\n",
            "Epoch 193/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 12.1729 - val_loss: 30.2236\n",
            "Epoch 194/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 13.2427 - val_loss: 32.5591\n",
            "Epoch 195/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 14.7511 - val_loss: 31.5089\n",
            "Epoch 196/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 14.8432 - val_loss: 27.6498\n",
            "Epoch 197/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 12.2802 - val_loss: 25.8169\n",
            "Epoch 198/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 14.1092 - val_loss: 28.0380\n",
            "Epoch 199/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 11.9471 - val_loss: 30.7831\n",
            "Epoch 200/200\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 12.0066 - val_loss: 27.5975\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 23.8609\n",
            "Feedforward Neural Network Model Mean Squared Error: 27.5975399017334\n"
          ]
        }
      ],
      "source": [
        "# Feedforward Neural Network\n",
        "\n",
        "def build_feedforward_model(input_shape,num_neuron,num_layer):\n",
        "    model=tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
        "    for _ in range(num_layer):\n",
        "        model.add(tf.keras.layers.Dense(num_neuron,activation='relu'))\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(1,activation=None))\n",
        "    return model\n",
        "\n",
        "num_layer=3\n",
        "num_neuron=64\n",
        "\n",
        "model_ffnn=build_feedforward_model(X_train.shape[1:],num_neuron,num_layer)\n",
        "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss=\"mean_squared_error\"\n",
        "model_ffnn.compile(optimizer=optimizer,loss=loss)\n",
        "history_ffnn=model_ffnn.fit(X_train,y_train,epochs=200,batch_size=32,validation_data=(X_test,y_test))\n",
        "mse_ffnn=model_ffnn.evaluate(X_test,y_test)\n",
        "print(f\"Feedforward Neural Network Model Mean Squared Error: {mse_ffnn}\")\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bonus Assignment \n",
        "* Try solving one more random dataset from kaggle/tensorflow datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cuyf9f0TE7wX",
        "outputId": "a6cc4902-f834-4991-9ae7-0023b25c115a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
